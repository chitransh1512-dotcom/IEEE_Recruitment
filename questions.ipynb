{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GG_6QiC9kKAi"
      },
      "outputs": [],
      "source": [
        "# [Q1]We don’t use `label = int(f)` because neural networks usually require one-hot encoded labels.\n",
        "# One-hot encoding converts a class index into an array with 1 at the correct class position and 0 elsewhere.\n",
        "# For 10 classes, this gives a label array of 10 elements, which is needed for Cross Entropy Loss.\n",
        "\n",
        "# [Q2] We convert to a NumPy array because neural networks and most ML libraries\n",
        "# expect inputs in array form for efficient numerical computations.\n",
        "# NumPy arrays allow vectorized operations, fast matrix multiplications, and are compatible\n",
        "# with libraries like TensorFlow and PyTorch.\n",
        "\n",
        "# [Q3] Selects only the first channel of each image.\n",
        "# Converts shape from (num_samples, height, width, channels) to (num_samples, height, width), e.g., RGB → single-channel.\n",
        "\n",
        "# [Q4] Flattens each image into a 1D vector. Neural networks expect input as (num_samples, features).\n",
        "\n",
        "# [Q5] Learning rate determines how much we update weights in each iteration of gradient descent.\n",
        "\n",
        "# [Q6] Weights are shaped (num_features, num_classes) to map inputs to outputs;\n",
        "# biases are shaped (1, num_classes) to add a constant per output neuron.\n",
        "\n",
        "# [Q7] Broadcasting allows adding the bias vector to every input sample automatically, without explicit loops.\n",
        "\n",
        "# [Q8] np.random.randn generates a matrix with samples from a standard normal distribution.\n",
        "# Shape of weight matrix is usually (num_features, num_classes).\n",
        "\n",
        "# [Q9] Activation functions introduce non-linearity, enabling the network to model complex patterns.\n",
        "\n",
        "# [Q10] Softmax converts raw scores (logits) into probabilities that sum to 1 across classes, useful for classification.\n",
        "\n",
        "# [Q11] Loss functions measure how far predictions are from true labels, guiding weight updates.\n",
        "\n",
        "# Forward Pass\n",
        "# input_list: (784, n) - n is the number of images\n",
        "# returns: (10, n) - n is the number of images\n",
        "\n",
        "# [Q12] Output has 10 elements per image because we have 10 classes; each element represents the probability of that class.\n",
        "\n",
        "# [Q13] Subtracting the mean centers the data around zero, which improves training stability and convergence.\n",
        "\n",
        "# [Q14] Softmax is used here to convert logits to probabilities, so the outputs can be interpreted as class probabilities.\n",
        "\n",
        "# [Q15] We do a forward pass here to compute the outputs for the current inputs\n",
        "# with the latest weights and biases. This is necessary because the network's parameters\n",
        "# may have been updated during training; we can't just reuse old outputs.\n",
        "\n",
        "# [Q16] The validation dataset is a separate set of data not used for training.\n",
        "# It is used to evaluate how well the model generalizes,\n",
        "# meaning how well it performs on new, unseen data instead of just memorizing the training set.\n",
        "\n",
        "# [Q17] Model parameters include:\n",
        "# - input_neurons: number of input features per sample\n",
        "# - hidden_neurons: number of neurons in the hidden layer(s)\n",
        "# - output_neurons: number of classes or output values\n",
        "# - learning_rate: step size for updating weights during training\n",
        "# - epochs: number of complete passes through the training dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hG5_4WYRkQb0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}